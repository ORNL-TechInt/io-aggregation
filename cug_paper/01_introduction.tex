\section{Introduction}
\label{sec:intro}

Most HPC applications' I/O exhibit a very `bursty' pattern. That is, there will be long periods with no file output while the application is in a compute phase, followed by a period of intense output and no computation when the application writes its checkpoint or output files.  At ORNL, the filesystem is a center-wide resource shared with multiple systems including a Cray XK7, a Cray XC30, analysis clusters, and WAN-connected data mover nodes. Each of these competing systems place a varying load on the filesystem and thus the performance that individual users see can vary dramatically within a job and between jobs.

Having a sufficiently large, node-local, write-back cache could improve an application's performance in two ways.  First, if the cache is large enough to hold an entire write and the individual writes are far enough apart, then the application never has to wait on the filesystem.  It can write to the cache and then resume its computations while the cache drains to the filesystem.  Secondly, write performance should be more predictable. From the application's point of view, write speed would only limited by how fast data can be copied into the cache, and since the cache is local to the node, it is not effected by outside influences.

Titan, the Cray XK7 at Oak Ridge National Laboratory, has 18,688 compute nodes. Each node consists of a 16-core AMD Interlagos CPU, an NVIDIA GPU and 32GB DRAM. In addition, there is another 6GB of GDDR DRAM on each GPU card. Not surprisingly, some applications do not use all the resources on a compute node.

To start with, during 2014, approximately 50\% of the compute time on Titan was used on applications that did not use the GPU at all.\cite{oareport}
There may be good reasons for this.  Utilizing the GPU requires changes to the application and it is possible the developers have not had the time or funding to do that.  It is also possible that the type of computations the application performs simply do not lend themselves to GPU-based acceleration (i.e. not enough exposed parallelism).  Regardless of why, it is clear that not all applications are benefiting from the GPU.

Secondly, the Interlagos has eight Bulldozer units, each of which has two full integer units and a shared floating-point unit. Some users with floating-point intensive applications run jobs using only eight processes per node (one process per Bulldozer) to avoid oversubscribing the floating-point units. This actually leaves eight integer cores available so long as they primarily execute integer instructions. 

The unused GPU memory and integer cores can be used to implement a write-back cache without taking resources away from the main application.  This paper demonstrates a technique for providing such a cache to applications.   Runs were made on Titan using a synthetic benchmark to demonstrate performance both without such a cache and with a cache that utilizes DRAM on the GPU card. The results show definite performance benefits.
 
%
% Everything below is commented out.  It's text I've toyed with using, but decided not to for whatever reason
%
\begin{comment}

Specifically, though AMD markets the processor as having 16 cores, it actually has only 8 floating-point units. As such, some applications that are floating-point intensive run with only 8 processes per node. This actually leaves 8 cores available so long as they primarily execute integer instructions. Furthermore, some applications do not make use of the GPU and thus it, and the ram attached to it, also go unused.

Since the GPU memory is accessed via the PCIe bus, it is generally not practical to use it as part of a process's main memory. Its bandwidth is too low and its latency is too high. However, when compared to filesystem performance - even a very fast parallel filesystem - accessing this memory is still very fast. Tests on Titan show that writing to GPU memory is at least eight times faster than writing to the filesystem, and considerably better than that if the filesystem is under heavy load. This suggests that using this memory as a large write-back cache for file I/O might help application runtimes significantly.

In HPC applications, most filesystem writes follow a very ?bursty? pattern. That is, there will be long periods with no file output while the application is in a compute phase, followed by a short period of intense output when the application writes its checkpoint or output files. Furthermore, at ORNL, the filesystem is a shared resource and other users will also be putting a varying load on it. This means that the performance that a user sees at any given time will likely vary dramatically.

Having a sufficiently large write-back cache would alleviate both of these problems. Assuming the cache is large enough, then the `bursty' nature of the output pattern can be smoothed out since there will be time for the traffic to drain from the cache before the next burst arrives. Equally importantly, write performance should be more predictable. From the application's point of view, write speed is only limited by how fast data can be copied into the cache, and that is not effected by outside influences. The GPU memory and integer cores can be used to implement such a cache without taking resources away from the main application.

This paper demonstrates a technique for providing such a write-back cache to applications. We use a synthetic benchmark to demonstrate performance without such a cache, with a cache that utilizes regular system ram and with a cache that utilizes ram on the GPU card. Performance benefits and any potential trade-offs and caveats are discussed.


 
 Since the GPU memory is accessed via the PCIe bus, it is generally not practical to use it as part of a process's main memory. Its bandwidth is too low and its latency is too high. However, when compared to filesystem performance - even a very fast parallel filesystem - accessing this memory is still very fast. Tests on Titan show that writing to GPU memory is at least eight times faster than writing to the filesystem, and considerably better than that if the filesystem is under heavy load. This suggests that using this memory as a large write-back cache for file I/O might help application runtimes significantly.


\end{comment}
