\section{Conclusion}
\label{sec:conclusion}

For the first test series (8 ranks/node), the results show clear performance improvement compared to writing directly to the filesystem.   More data can be cached in the GPU's memory than in the Lustre client-side cache and the average write speed is, surprisingly, slightly higher.  It is, however, necessary to use relatively large writes.  As shown in Figure \ref{fig:transfer_bw}, the transfer bandwidth between the host and GPU is relatively poor for small transfers.  Comparing Figure \ref{fig:results_base_8} and Figure \ref{fig:results_8_nobars},  it can be seen that write sizes must be at least 4MB before transfers to the GPU memory outperform writes to the Lustre cache.

Note that these tests were run with file striping left at its default value of 4.  In theory, writes to the filesystem could be improved by increasing the number stripes for each file, but that requires more system memory, which necessarily leaves less for the application.


The results for the second test series (16 ranks/node) are less clear.  As with the first test series, write sizes need to be at least 4MB Looking closely at Figure \ref{fig:results_base_16} and Figure \ref{fig:results_16_nobars}, it appears that write sizes that can fit into GPU RAM perform slightly better than just writing to disk.  However, write sizes that exceed the capacity of the GPU RAM are actually a little slower than writing straight to the filesystem.

There are several other complications to consider, though.
First, Since the test application did no actual computation, there were plenty of cores available for the daemon to execute on.  In a real HPC application, this wouldn't be the case and the application's performance would almost certainly be negatively affected by the context switches.  On the other hand, an application that is running 16 ranks/node is unlikely to leave much system memory free for use by the Lustre client-side cache.  So, caching writes in the GPU memory might offset the disruption caused by oversubscribing the cores.  Whether or not this technique provides a net performance improvement in such a configuration will probably depend on the particular application.

An interesting compromise might be to have the application run 15 ranks/node instead of 16.  That will leave 1 core available for the daemon.  Both threads could be pinned to that core, with the message processing thread given higher priority.  It would also be possible to use the  core specialization option to aprun to schedule OS tasks on that core.  Compared to an application running lots of MPI collectives, the daemon isn't particularly sensitive to short interrupts like that.  This configuration hasn't been tested and the authors are unaware of any application that routinely runs with 15 ranks/node.

In general, the results show that using the GPU memory for cache provides approximately the same performance as the Lustre client-side cache.  The difference is that in order to benefit from the Lustre cache, the writes must be spread across a sufficient number of OST's and the memory must actually be available.  Whether or not these conditions can be met depends on the particular application.  

