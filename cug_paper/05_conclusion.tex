\section{Conclusion}
\label{sec:conclusion}

For the first test series (8 ranks/node), the results show a clear performance improvement compared to writing directly to the filesystem.   More data can be cached in GPU memory than in the Lustre client-side cache and the average write speed is, surprisingly, slightly higher.  It is, however, necessary to use relatively large writes.   Comparing Figure \ref{fig:results_base_8} and Figure \ref{fig:results_8_nobars},  it can be seen that write sizes must be at least 4MB before transfers to GPU memory outperform writes to the Lustre cache.

Note that these tests were run with file striping left at its default value of 4.  In theory, writes to the filesystem could be improved by increasing the number stripes for each file, but that requires more system memory, which necessarily leaves less for the application.

The results for the second test series (16 ranks/node) are less clear.  As with the first test series, write sizes need to be at least 4MB Looking closely at Figure \ref{fig:results_base_16} and Figure \ref{fig:results_16_nobars}, it appears that write sizes that can fit into GPU memory perform slightly better than just writing to disk.  However, write sizes that exceed the capacity of the GPU's memory are actually a little slower than writing straight to the filesystem.

There are several other complications to consider, though.
First, Since the test application did no actual computation, there were plenty of cores available for the daemon to execute on.  In a real HPC application, this wouldn't be the case and the application's performance would almost certainly be negatively affected by the context switches.  On the other hand, an application that is running 16 ranks/node is unlikely to leave much system memory free for use by the Lustre client-side cache.  So, the improvement from caching writes in GPU memory might offset the disruption caused by oversubscribing the cores.  Whether or not this technique provides a net performance improvement in such a configuration will probably depend on the particular application.

An interesting compromise might be to have the application run 15 ranks/node instead of 16.  That will leave 1 core available for the daemon.  Both threads could be pinned to that core, with the message processing thread given higher priority.  It would also be possible to use the  core specialization option to aprun to schedule OS tasks on that core.  Compared to an application running lots of MPI collectives, the daemon isn't particularly sensitive to short interrupts like that.  This configuration hasn't been tested and the authors are unaware of any application that routinely runs with 15 ranks/node.

In general, the results show that using the GPU memory to cache filesystem writes provides approximately the same performance as the Lustre client-side cache.  The difference is that the Lustre cache uses system memory which may not actually be available.  Even if it is, the user must ensure that writes are spread across a sufficient number of OST's to make sue of it.  Neither of these conditions apply when using GPU memory as a cache.


%Figures \ref{fig:results_base_8} and \ref{fig:results_base_16} show that Lustre client cache can provide good write performance up to 256MB write sizes.  However, that presumes that there is, in fact, sufficient system memory available on the node \emph{and} that data is being written to a sufficient number OST's.  If a 'real' application only wrote to a single file, the Lustre cache would not provide nearly as large a benefit unless the user took additional steps to increase the stripe count of the file.  It's worth noting that if an application used the techniques demonstrated in this paper, it would see improved write performance regardless of how many OST's were used and how much system ram was available for cache.

