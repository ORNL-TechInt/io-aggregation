\section{Future Work}
\label{sec:future}

The authors work in a group that is not dedicated to pure research; part of the group's mandate is to deploy useful software in the production environment.  As such, the most important piece of future work is to convert this demonstration code into a package that can actually be used by other applications running on Titan.  In the introduction, the authors noted that one of the reasons some applications have not been converted to use the GPU for calculations is that the scientific applications' developers have not had time and/or funding.  With that in mind, it is impractical to expect those same developers to make major changes in order to use this code.  

The authors are considering a number of methods of packaging this code to make it easy for application developers to integrate into their own applications.  One possibility is to intercept certain POSIX function calls, such as \texttt{open()} and \texttt{write()}, among others.  While somewhat tricky to implement, this technique has been used with good success by the Mercury Posix project.\cite{mercury_posix_demo}  A second possibility is to modify one or more I/O libraries that are popular with applications, such as the NetCDF library, to make use of this code.  This has the advantage of requiring zero code changes by the application developers.  Properly maintaining the modified I/O libraries could be a real problem, though. A final possibility is to integrate this code into the Functional Partitioning project that the authors and their colleagues are working on.\cite{Li:2010:FPO:1884643.1884686}  Obviously, the authors could choose more than one of these alternatives.


Besides deploying this code in a production environment, there are a few features the authors would like to add.  The simplest is to add a user-controlled option to limit the amount of GPU memory the daemon will try to allocate.  As currently written, the daemon will allocate all the memory it can.  That could cause problems if the application wants to use the GPU for calculations.  By allowing the user to set a hard limit on the amount of memory the daemon will use, it becomes possible for this software to coexist with applications that use the GPU for calculations.

Another feature the authors would like to add is the ability to use regular system RAM in addition to the GPU RAM.  It is well known that some applications do not use all the RAM on the compute node.  It is debatable whether letting the daemon use the memory is better than simply letting the operating system use it as  Lustre client cache or OS page cache.  However, as was discussed in Section \ref{sec:analysis} and Section \ref{sec:conclusion}, the Lustre client-side cache has fairly low limits on the amount of dirty pages it will allow.  (The Linux kernel page cache has similar low limits on dirty pages.)  Some very basic initial testing implies that allowing the daemon to allocate memory may be more useful than letting the OS use the same memory for page cache, but further testing is required to be certain.

Lastly, the next big supercomputer at ORNL has already been announced.  While some details have yet to be finalized, one thing that has been decided is that each node will have approximately 800GB of nonvolatile memory (NVM).\cite{summit_page}  
Exactly how applications will use this NVM memory is the subject of much research and debate. With only minor changes, this software could write to the NVM instead of (or in addition to) the GPU memory.  Assuming the ease-of-use goals mentioned above are met, the authors feel this would allow application developers to quickly modify their applications to use the NVM as a large burst-buffer, even as those developers look for other, more effective ways to make use of the hardware.

